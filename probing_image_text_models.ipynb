{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9h3kNTyMOAC"
      },
      "source": [
        "##Probing Image-Text Models\n",
        "\n",
        "\n",
        "This Colab evaluates pretrained image--text models (in a zero-shot way) with respect to fine-grained **s**ubject, **v**erb, and **o**bject understanding using the [SVO-Probes](https://arxiv.org/abs/2106.09141) dataset. \n",
        "\n",
        "The SVO-Probes dataset provides two positive and negative images for a given sentence.  The negative image differs from the positive one with respect to either subject, verb, or object. Here are some examples of the SVO-Probes dataset: \n",
        "\u003cfigure\u003e\n",
        "\u003ccenter\u003e\n",
        "\u003cimg src='https://storage.googleapis.com/dm-mmt-models/svo_probes_examples.png'\u003e\n",
        "\u003cfigcaption\u003eExamples from the SVO-Probes dataset\u003c/figcaption\u003e\u003c/center\u003e\n",
        "\u003c/figure\u003e\n",
        "\n",
        "\n",
        "Given a sentnece, we test if a model can correctly classify both positive and negative images; the notebook will output raw numbers on the SVO-Probes dataset and generate bar charts to visualize the results.\n",
        "\n",
        "\n",
        "### Setting Up Your Models\n",
        "\n",
        "You can find the SVO-Probes raw data [here](svo_probes.csv). Each row of the data contains of two datapoints, the `\u003csentence,positive-image\u003e` and` \u003csentence,negative-image\u003e` pairs. Each image is identfied by a url and a unique id: `pos_image_id` (`pos_url`) or `neg_image_id` (`neg_url`) to mark the positive and negative images, respectively.\n",
        "\n",
        "To evaluate your models on SVO-Probes, for each image and sentence pair, it needs to output whether or not the image and text match. \n",
        "\n",
        "You can also download the images using their urls listed [here](image_urls.txt). \n",
        "\n",
        "### Using the Notebook\n",
        "\n",
        "To use the notebook, evaluate your model for all image and sentence pairs. Save these results in a json dictionary with key:value pairs formatted as `sentence|imaage_id`: 1 if sentence matches the image (and 0 if does not match).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDN56xIjtGxj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxVO1qFuPcc"
      },
      "source": [
        "Reading the image-sentence scores from the a json file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8-HPRRUuXFG"
      },
      "outputs": [],
      "source": [
        "def scores(path):\n",
        "  id_to_scores = {}\n",
        "  d = json.load(\n",
        "      gfile.Open(\n",
        "          path,\n",
        "          'r'))\n",
        "  # id --\u003e score\n",
        "  for item in d:\n",
        "    id_to_scores[item] = float(d[item])\n",
        "  return id_to_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIr0wumAxxO9"
      },
      "source": [
        "Read a model's scores on each sentence-image pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKvhXUCSv7ua"
      },
      "outputs": [],
      "source": [
        "def get_score_dataframe(path, df):\n",
        "  sentences = [item for item in df.sentence.values]\n",
        "  pos_image_id = [item for item in df.pos_image_id.values]\n",
        "  neg_image_id = [item for item in df.neg_image_id.values]\n",
        "  idxs = [item for item in df.index]\n",
        "\n",
        "  pair_to_scores = scores(path)\n",
        "\n",
        "  pos_scores = []\n",
        "  neg_scores = []\n",
        "  count = 0\n",
        "  for idx, sentence, pos_image_id, neg_image_id in zip(idxs, sentences, \n",
        "                                                       pos_image_id, \n",
        "                                                       neg_image_id):\n",
        "    neg_key = re.sub(' +', ' ', '%s|%d' % (sentence.lower(), neg_image_id))    \n",
        "    pos_key = re.sub(' +', ' ', '%s|%d' % (sentence.lower(), pos_image_id))\n",
        "    if (pos_key in pair_to_scores) and (neg_key in pair_to_scores):\n",
        "      pos_scores.append(pair_to_scores[pos_key])\n",
        "      neg_scores.append(pair_to_scores[neg_key])\n",
        "    else:\n",
        "      df = df.drop([idx])\n",
        "    count += 1\n",
        "  \n",
        "  df['pos_scores'] = pos_scores\n",
        "  df['neg_scores'] = neg_scores\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErtUVIPovfKp"
      },
      "source": [
        "## SVO-Probes: How do models perform on subject, verb, and object pairs?\n",
        "\n",
        "Read our raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5nWtl_Lu66o"
      },
      "outputs": [],
      "source": [
        "# SVO Probes dataset\n",
        "!wget  https://storage.googleapis.com/dm-mmt-models/svo_probes.csv  --no-check-certificate  -P '/tmp'\n",
        "%ls /tmp\n",
        "df = pd.read_csv(gfile.Open('/tmp/svo_probes.csv', 'r'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3pLiSb25RAI"
      },
      "source": [
        "Computing accuracy across different types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4de6a4IViAE"
      },
      "outputs": [],
      "source": [
        "# Change this path to include the scores from your model\n",
        "!wget https://storage.googleapis.com/dm-mmt-models/mmt_cc_svo_results.json  --no-check-certificate  -P '/tmp'\n",
        "json_path = '/tmp/mmt_cc_svo_results.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81_jsJF05QQN"
      },
      "outputs": [],
      "source": [
        "def accuracy(frame):\n",
        "  neg = frame[['sentence', 'neg_image_id', 'neg_scores']].drop_duplicates()\n",
        "  pos = frame[['sentence', 'pos_image_id', 'pos_scores']].drop_duplicates()\n",
        "\n",
        "  neg_acc = np.mean([item == 0 for item in neg['neg_scores'].values])\n",
        "  pos_acc = np.mean([item == 1 for item in pos['pos_scores'].values])\n",
        "  # macro\n",
        "  acc = (neg_acc + pos_acc)/2\n",
        "  return acc, pos_acc, neg_acc\n",
        "\n",
        "\n",
        "data_df = get_score_dataframe(json_path, df)\n",
        "\n",
        "subj_neg_df = data_df[data_df['subj_neg'] \u0026 ~data_df['obj_neg'] \n",
        "                      \u0026 ~data_df['verb_neg']]\n",
        "verb_neg_df = data_df[data_df['verb_neg'] \u0026 ~data_df['obj_neg'] \n",
        "                      \u0026 ~data_df['subj_neg']]\n",
        "obj_neg_df = data_df[data_df['obj_neg'] \u0026 ~data_df['verb_neg'] \n",
        "                     \u0026 ~data_df['subj_neg']]\n",
        "\n",
        "all_df = pd.concat([subj_neg_df, verb_neg_df, obj_neg_df])\n",
        "\n",
        "acc_all, pos_acc_all, neg_acc_all = accuracy(pd.concat([subj_neg_df, \n",
        "                                                        verb_neg_df, \n",
        "                                                        obj_neg_df]))\n",
        "acc_subj, pos_acc_subj, neg_acc_subj = accuracy(subj_neg_df)\n",
        "acc_verb, pos_acc_verb, neg_acc_verb = accuracy(verb_neg_df)\n",
        "acc_obj, pos_acc_obj, neg_acc_obj = accuracy(obj_neg_df)\n",
        "\n",
        "results = [['All', acc_all, pos_acc_all, neg_acc_all], \n",
        "           ['Subj', acc_subj, pos_acc_subj, neg_acc_subj],\n",
        "           ['Verb', acc_verb, pos_acc_verb, neg_acc_verb],\n",
        "           ['Obj', acc_obj, pos_acc_obj, neg_acc_obj]]\n",
        "\n",
        "results_df = pd.DataFrame.from_records(results, \n",
        "                                       columns=['Type', 'Avg Accuracy', \n",
        "                                                'Pos Accuracy', 'Neg Accuracy'])\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BMANUPUG-Wj"
      },
      "source": [
        "Plot positive, negative, and average results for subjects, verbs, and objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufZEZqrhFeDh"
      },
      "outputs": [],
      "source": [
        "tmp_df = results_df\n",
        "tmp_df = tmp_df.rename(columns={\"Avg Accuracy\": \"Avg\", \n",
        "                                \"Pos Accuracy\": \"Pos\", \n",
        "                                \"Neg Accuracy\": \"Neg\"})\n",
        "\n",
        "melt_df = pd.melt(tmp_df, id_vars = \"Type\")\n",
        "melt_df = melt_df.rename(columns={\"variable\": \"Accuracy Type\", \n",
        "                                  'value': 'Accuracy', \n",
        "                                  'Type': 'Word Type'})\n",
        "_ = sns.barplot(data=melt_df, x='Accuracy Type', y='Accuracy', hue='Word Type')\n",
        "\n",
        "fig = plt.gcf()\n",
        "_ = fig.set_size_inches(10, 5, forward=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea21-vPBFTIx"
      },
      "source": [
        "Plot average, positive, or negative accuracy for different word types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m70WSt1hD0bs"
      },
      "outputs": [],
      "source": [
        "input_or_select = \"Avg Accuracy\"  # @param [\"Avg Accuracy\", \"Pos Accuracy\", \"Neg Accuracy\"]\n",
        "_ = sns.barplot(data=results_df, x='Type', y=input_or_select)\n",
        "\n",
        "fig = plt.gcf()\n",
        "_ = fig.set_size_inches(10, 5, forward=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Probing Image-Text Models",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
